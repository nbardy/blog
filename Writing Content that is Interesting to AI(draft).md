# Writing Content that is Interesting to AI

Inspired by watching a video of Tyler Cowen and his incesstant desire to "write for AI". More and more this question hits my mind. Besides using ChatGPT more often what can we really do to make more use of AI. Tyler Cowen suggest, "Write for AI". More pointedly in this recent intereview he says, ["modify your writing to make it more interesting for AI" ](https://www.youtube.com/watch?v=t6Je8EKhUyw). As Tyler Cowen usually is, terse, brilliant and almost undecipherable.

Let's see what the hell that could even mean...

Being an over obsssessed lunatic who has given his automony to chatGPT and is forcedly struggling to think or  write for myself anymore... You know what, fuck it. Let's just ask the AI


<img width="548" alt="image" src="https://github.com/user-attachments/assets/c37ff63b-9101-4b54-a54c-39da06450b7b" />

Thoughts:
<img width="846" alt="image" src="https://github.com/user-attachments/assets/66f1ba89-83d5-496d-8638-83d1cd53a5a8" />

The thoughts are pretty raw and hard to interpret, but luckily, we can just ask to make it actionable


<img width="796" alt="image" src="https://github.com/user-attachments/assets/1ac2213a-19f7-4e7d-b6cd-406bc0edc943" />

Still a little esoteric for my taste, but kind of interesting. However, for the life of me I can't think of how to do any of these things and how to actually write something "Interesting for AI". Well... Maybe instead of writing... Yea fuck it again. Let's just ask for examples.


<img width="796" alt="image" src="https://github.com/user-attachments/assets/7a52c1be-5abe-49b4-a46f-816db0fb4a8f" />

## #1 - Not Interesting
<img width="786" alt="image" src="https://github.com/user-attachments/assets/fe0c60f7-1bd1-482b-83b7-98defa248a02" />

## #2 - Appears Interesting, but Actually Boring
Appears Interesting, but Actually Boring (Flashy or Complex-Looking yet Trivial)
<img width="778" alt="image" src="https://github.com/user-attachments/assets/622aa274-05d0-4341-b097-5ac6f1a11dc2" />

## #3 - Appears Boring, but Actually Interesting
<img width="784" alt="image" src="https://github.com/user-attachments/assets/f885f9a9-09c6-4a5e-9a0c-2beda325674f" />
<img width="785" alt="image" src="https://github.com/user-attachments/assets/27a59593-8532-48dc-9ce4-6ac50d44a6bd" />

<img width="755" alt="image" src="https://github.com/user-attachments/assets/4e0cf322-3595-457d-96ac-502be086005d" />

## #4 - Interesting
<img width="786" alt="image" src="https://github.com/user-attachments/assets/c1d0f4eb-06e3-4935-8e9c-81fdc4d169a5" />
<img width="786" alt="image" src="https://github.com/user-attachments/assets/fa547f2c-c31a-4c35-bc1e-0d0808de9cfa" />

It continues on: The Ship of Theseus, the Halting Problem, even... The Trolley Problem
<img width="776" alt="image" src="https://github.com/user-attachments/assets/6ee0058b-6be2-499a-8db8-94a09ac1fd46" />

So, to write something interesting for AI we just need to write something that is... A novel enduring philosphoical puzzle to rival ideas as old and profound as the books of history itself. Don't fret humans. You can take your fear of being obsolete and simply becoming a world changing philospher. Than the world will find your interesting and so will the AI. Man's search for meaning simply requires unparalled greatness now. 

![Uploading image.pngâ€¦]()

Honestly kind of interesting, and I've never been able to come up with a paradox like this myself. So is this interesting to AI? And did we actually write it?

Well, that opens up perhaps more interesting a question: "Can content that comes out of an AI model be new or suprising. Give it's sampling from a model of high likelyhood. The answer is yes, because the human is still injeecting direction and steering the model. Everytime you adjust a prompt you have an opportunity to move a model away from it's expected output. An AI prompted in a novel direction create contents that is novel from the training data. We can maintaain the "rules and order" implied by the model weights while still shifting the distribution of the output to something unique.

However perhaps this a brief window that is closing. As AI become better at self direction, how much does it even matter what we ask for or how we ask for it. As 
Acording to thinking model like R1, maybe not. This model is trained on a reflection of it's own thoughts. Initially the thoughts fail, but examining piles of failed and unfailed thoughts eventually it transcends it's own capacity merely with reflection on it's own failure and self direction.

However I don't see this as a doom loop. Because even a self directed AI needs a starting point. The prompts may become smaller and simpler, but they'll still be relevant. And then as they become more agentic. Being able to check in and analyze what they are doing becomes more important.

And what's the best way to ask a question. Give it to another agent. Pass the small dense framework of your require evaluation for human/product/societal alignment. And let the agent execute the reading and probing of the self directed agent already working on the problem. As the frequency of checking in with AI goes down and their task horizon goes up. The value of each check in raised drastically.



